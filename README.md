# ğŸš€ Data Science Salary Estimation  

This project builds a **data pipeline for estimating salaries for data science jobs** by scraping, cleaning, and enriching job postings from company websites and job boards. The end goal is to create a structured repository that can be used for **analytics, salary prediction, and trend analysis**.  

> âš ï¸ **Note on Data Privacy**  
> To respect the privacy policy of the source platforms, we do **not disclose the exact websites** being scraped. Furthermore, the extracted data is **manipulated, cleaned, and transformed** before being published here to avoid sharing raw proprietary content.  

---

## ğŸ“‚ Pipeline Overview  

1. **Scraping (âœ… Completed)** â€“ Automated collection of raw job postings with Selenium  
2. **Data Cleaning & Enrichment (ğŸ”„ In Progress)** â€“ Extracting skills, experience, industries, and refining numerical & categorical features  
3. **Exploratory Data Analysis (ğŸ“Š Upcoming)** â€“ Identifying trends in roles, seniority, required skills, and compensation  
4. **Modeling (ğŸ¤– Planned)** â€“ Predicting salaries and career paths based on job descriptions  
5. **Inferential Statistics (ğŸ“ Planned)** â€“ Deriving **confidence intervals** for data science roles  

---

## ğŸ”¹ 1. Scraping Phase  

### ğŸ¯ Goal  
To collect structured job posting data at scale for further analysis.  

### ğŸ“Œ Scope  
- **Target**: Company job boards & listings pages  
- **Extracted Features**:
  - url of the job post (for further investigation if needed)
  - Job title  
  - Company name & description  
  - Location & work setup (remote / hybrid / on-site)  
  - Posting date  
  - Salary (if available)  
  - Seniority level & role indicators  
  - Company details (size, revenue, headquarters, ownership)  
  - Full job description  

### âš™ï¸ Technical Details  
- **Framework**: [Selenium](https://www.selenium.dev/) for browser automation  
- **Browsers Supported**: Firefox
- **Dynamic Content**: Handled with explicit waits, scrolling, and pop-up management  
- **Robustness**:  
  - URL validation before scraping  
  - Error handling for missing or inconsistent fields  
  - Configurable depth (`pages` parameter).  

### ğŸ“¤ Output  
- Raw job data stored as a **pandas DataFrame**  
- Exported to:  data/raw_data.csv


### â–¶ï¸ Quick Start  
Run the crawler directly with:  

```bash
python crawl.py --pages 5 --url "https://example.com/jobs"

Arguments:

- --pages â†’ Number of listing pages to scrape

- --url â†’ Starting job board URL

## ğŸ“Œ Next Steps  

- Cleaning and enrichment of scraped data  
- Extraction of experience requirements, technical skills, and industry classification  
- Preparing the dataset for downstream **EDA & modeling** 
- Preprocessing for machine learning readiness
- Model training for salary prediction
- Inferential analysis for confidence intervals

---

## âš–ï¸ License  

This project is licensed under the **MIT License** â€“ you are free to use, modify, and distribute it.  

---

## âœ¨ Acknowledgments  

- [Selenium](https://www.selenium.dev/) for automated web scraping  
- [pandas](https://pandas.pydata.org/) for data handling  
- Future integrations with **LLMs** for advanced information extraction  

